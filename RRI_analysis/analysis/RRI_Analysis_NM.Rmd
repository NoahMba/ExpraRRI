---
title: 'Repeated Recording Illusion: Analysis I - IV'
author: 'Noah Mba & EXPRA Group 6'
output:
  html_document:
    df_print: paged
---
# Analysis preparation
##### 0.1 Set Working Directory
1. Set working directory to file path. (Recall, `Session > Set Working Directory > To Source File Location`.) This script should be sitting in a folder structure as follows:

`RRI_analysis/analysis/RRI_analysis_I.Rmd`

### 0.2 Load packages
```{r}
## We will need the following packages
packages <- c("tidyverse", "sjPlot", "ordinal", "ggeffects", "psych", "lme4", "RColorBrewer", "RVAideMemoire", "GGally", "pwr")

## Now load or install & load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
options(dplyr.summarise.inform = FALSE)
```

### 0.3 Power analysis by Ava Kiai
```{r}
# I checked to see how many participants would be needed to find differences
# in liking for different levels of prestige suggestion. 

# Linear Model: N to detect differences in liking for levels of prestige
  r2_classical = .16 # from Anglada-Tort 2017
  r2_pop = .28 # from Anglada-Tort 2017
  
  # function to calculate f2 from model R2
  f2 <- function(r2) r2/(1-r2)
  
  # calculate f2 for each model
  f2_classical = f2(r2_classical)
  f2_pop = f2(r2_pop)

  pwr.f2.test(u = 3-1, f2 = f2_classical, sig.level = 0.05, power = 0.95)
  pwr.f2.test(u = 3-1, f2 = f2_pop, sig.level = 0.05, power = 0.95)
  # where u and v are the numerator and denominator degrees of freedom. We use f2 as the effect size measure.

# Results: (v+u+1 = ) 71 for classical, 34 for popular music
```

### 0.4 Load Data
Load packages and data. The file `data.csv` contains all data we will use. 
(The file `demographic_data.csv` 
contains demographic info such as the Goldsmiths Musical Sophistical Index we implemented at the start of the experiment. We can reference this when we need it later on.) 

```{r message=FALSE, warning=FALSE}
# set path to current directory
data_path <- c('../data')

# load data
data <- read.csv(file.path(data_path,'data.csv')) %>% 
  mutate(condition = factor(condition, levels = c("baseline","low","high")))

# How many participants do we have in total?
print(paste("N =", length(unique(data$ID))))
head(data)
```

### 0.5 Description of sample
Let's look at the distribution of age and gender in our sample
```{r}
age_dat <- data %>% dplyr::group_by(ID, wm) %>% summarise(age = unique(age))
psych::describe(age_dat$age)
psych::describe(age_dat[age_dat$wm == 1,]$age)
psych::describe(age_dat[age_dat$wm == 0,]$age)


sex_dat <- data %>% dplyr::group_by(ID, wm) %>% summarise(sex = unique(sex)) 
sex_dat$sex <- factor(sex_dat$sex, labels = c("male", "female"))
table(sex_dat$sex)
table(sex_dat[sex_dat$wm == 1,]$sex)
table(sex_dat[sex_dat$wm == 0,]$sex)
```

# Analysis I- Liking Ratings
In this section we're going to begin our analysis of the Repeated Recording Illusion Experiment. We will visualize our data and answer our Hypothesis #3.

## Roadmap
We'll perform the following steps:

1. Outlier Removal: Remove trials where participants in the working memory condition did not recall at least 4/8 characters correctly.

2. Aggregate and plot data related to our Hypothesis 3: Did WM Load, Explicit Information, and Genre have an influence on liking ratings? 

3. Perform two ordinal logistic regressions (mixed-effects models) to respond to Hypothesis 3. Recall that our outcome variable is `mean liking` and our predictors are `working memory load` and `explicit information`  and that we will (first) run a separate model for each `genre`.


## I.1 Outlier Removal/Data Exclusion: Failed WM Trials 
First, let's remove those participants who did not get a number of character right in the working memory task.

**Decision Time:** We decided that participants should correctly reproduce at least 4 letters.

```{r}
# minimum correct
N = 4

# add row number as column, so we can find the rows we want to keep later
  data_row <- tibble::rowid_to_column(data, "rown")
  
  # get only rows of data from wm condition, and select only relevant columns
  wm_1 <- data_row[!is.na(data_row$probe) & data_row$wm==1,] 
  
  # save the rest for later (adding back together the relevant rows)
  wm_0 <- data_row[is.na(data_row$probe) | data_row$wm==0,] 

# select only relevant columns for this task
wm_1a <- wm_1 %>% dplyr::select(rown, ID, probe, wm_response)

# change responses to upper case, so that we can compare them easily to the probe
wm_1a$wm_response <- toupper(wm_1$wm_response) 

# add a column of zeros to the right side of the dataframe, this is where you will store the answers to
# whether or not someone got enough characters correct
wm_1a$check <- 0

# Here's a little function to separate responses and prompts into a string of unique characters. (Since
# our probes never have repeating letters, we want to make sure we only "give points" for unique 
# character matches.)
uniqchars <- function(x) unique(strsplit(x, "")[[1]]) 

# ----
# For each probe, compare with the response, and set the check value to 1 if matches > N.
# What cutoff N should we use?
for (n in 1:nrow(wm_1a)) {
  correct <- sum(uniqchars(wm_1a$probe[n]) %in% uniqchars(wm_1a$wm_response[n]))
  if (correct > N-1) {
    wm_1a$check[n] <- 1
  }
}

# keep only rows where check was passed
wm_1_keep <- wm_1[wm_1a$check==1,]

print(paste("Percentage of correct trials:",round(sum(wm_1a$check==1)/nrow(wm_1) * 100,2),"%"))
print(paste("That's",(nrow(wm_1)-nrow(wm_1_keep))/6,"trials we've removed."))

# combine the two sets, kept rows from working memory condition, and the rest 
data_or_rm <- rbind(wm_1_keep, wm_0) %>% arrange(rown)

print(paste("N =", length(unique(data_or_rm$ID))))

```

Now, let's check how many participants and trials we have in each cell: 
```{r}
data_check <- data_or_rm[!is.na(data_or_rm$condition),] %>% group_by(ID, prime_order) %>% dplyr::slice(1) 
data_check %>% as.matrix() %>% as.data.frame() %>% 
  group_by(wm, genre, prime_order) %>% tally() %>% arrange(wm, genre, prime_order)
```

## I.2 Principal component analysis
For our main hypotheses, we want to aggregate the ratings on the four questions into a single  measurement of musical liking. This is only appropriate if the items measure a one-dimensional  construct. We will use a principal component analysis (PCA) to test this.
```{r}
# First, let's reduce the data down to only what we need for this section. 
# We only need those rows of data where the ratings column is not empty (not NA).
rating_data <- data_or_rm[!is.na(data_or_rm$question_n),]

# Create a data.frame for PCA, with each rating question as a column
interpretation_scores <- rating_data[rating_data$question_n == "Interpretation",]
timing_scores <- rating_data[rating_data$question_n == "Timing/Rhythm",]
tone_scores <- rating_data[rating_data$question_n == "Tone Quality",]
expressiveness <- rating_data[rating_data$question_n == "Expressiveness",]

pca_data <- data.frame(ID = interpretation_scores$ID, wm = interpretation_scores$wm,
                       Condition = interpretation_scores$condition, genre = interpretation_scores$genre,
                       Interpretation = interpretation_scores$rating,
                       Timing = timing_scores$rating, Tone = tone_scores$rating,
                       Expressiveness = expressiveness$rating)

items <- c("Interpretation", "Timing", "Tone", "Expressiveness")

Parallelanalyse <- fa.parallel(pca_data[, items], ylabel = "Eigenwert", fa="pc")
# Scree plot suggests a one-component structure

(Parallelanalyse$pc.values[1] / length(items)) * 100
# First component explains 70.78% of variance

eigendat <- rbind(data.frame(Factor=c(1:length(items)), Eigenwert=Parallelanalyse$pc.values,
                             Typ = "Observed Eigenvalues"),
                  data.frame(Factor=c(1:length(items)), Eigenwert=Parallelanalyse$pc.sim,
                             Typ = "Simulated Eigenvalues"))

ggplot(eigendat, aes(x=Factor, y=Eigenwert, shape=Typ)) +
  geom_line()+
  geom_point(size=2)+
  scale_y_continuous(name='Eigenwert', limits = c(0,3))+
  scale_x_continuous(name='Factor')+
  scale_shape_manual(values=c(16,1)) 
ggsave("Screeplot_PCA.jpg", width = 7, height = 5)
```

## I.3 Plotting: How were ratings modulated by working memory load, levels of explicit information, and genre? (Each question separate)

Summary statistics:
Provide the code to generate summary statistics. Keep in mind that Likert data is ordinal data, so aspects like range and median, in addition to the mean, can be important. Summarise the results in a table below.
```{r}
rating_summary_genre <- rating_data %>% group_by(genre, question_n) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)),
                                                                min_rating = min(rating),
                                                                max_rating = max(rating),
                                                                md_rating = median(rating),
                                                                iqr_rating = IQR(rating))

rating_summary_overall <- rating_data %>% group_by(genre) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)),
                                                                min_rating = min(rating),
                                                                max_rating = max(rating),
                                                                md_rating = median(rating),
                                                                iqr_rating = IQR(rating))
```

## I.4 Plotting for combined liking ratings
Now let's get the mean ratings over participants for each question about liking, and overall. We
want to get the `mean`, `standard deviation`, and `standard error of the mean`. 
We need the standard deviation to get the standard error, but the standard error is often what 
we plot (the "whiskers") around the mean.

This will not be reported.
```{r message=FALSE, warning=FALSE}
# ratings per wm group, level of explicit information, and question
rating_summary <- rating_data %>% group_by(wm, condition, question_n) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)))

# mean, se ratings ~ explicit information level * working memory * question
ggplot(rating_summary) + 
  geom_point(aes(question_n, mean_rating, color = condition)) + # adds a point for each obs (row)
  geom_errorbar(aes(x = question_n, y = mean_rating, # add errorbars, given specs on ymin/ymax
                   ymin = mean_rating-se_rating, ymax = mean_rating+se_rating, color = condition),
                width = 0.2) + # controls width of the lateral bar
  # line connects points based on value in "group" (if group == 1, there's only one level)
  geom_line(aes(question_n, mean_rating, group = condition, color = condition)) + 
  # "facets" split the graph based on the variables given, so here it splits based on levels of "wm"
  # dot(.i) = means data that is already in the pipe
  facet_grid(. ~ wm, labeller = as_labeller(c(`0` = "no load", `1` = "wm load"))) +
                    # to change the label in the strip on the top, assign to the original value a new str   
  # above, we have told R to assign colors based on the variable "condition"
  # here, we tell it what color palette to use, and what to name the legend
  scale_color_brewer(palette = "Dark2", name = "explicit information") +
  # use a clean, white theme
  theme_classic() +
  # theme adjustments: angle the x-axis tick labels so that they do not overlap
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  # change y-axis title and remove x-axis title
  labs(y = "liking ratings", x = NULL)
# average ratings over questions

```

Now let's plot the mean ratings:
(Also not included in the report)
```{r message=FALSE, warning=FALSE}
ratings_mu_summary <- rating_data %>% group_by(wm, condition) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)))
# mean, se ratings ~ explicit information level * working memory 
ggplot(ratings_mu_summary, mapping = aes(x = wm, y = mean_rating, color = condition)) + 
  geom_point(size = 3) +
  geom_line(aes(group = condition)) + 
  geom_errorbar(aes(y = mean_rating, 
                    ymin = mean_rating - se_rating, ymax = mean_rating + se_rating, color = condition),
                width = 0.02) +
  scale_x_discrete(limits = c(0,1), labels = c("no load", "load"), name = "working memory") +
  scale_color_brewer(palette = "Dark2", name = "explicit information") +
  theme_classic() +
  labs(y = "mean liking ratings")

```

**Stop & Reflect:** What do you see the above figure? Can you describe the results? Is there an interaction? What is missing from the figure?

Now let's add genre to the mix: 
```{r message=FALSE, warning=FALSE}
genre_summary <- rating_data %>% group_by(wm, condition, genre, question_n) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)))

# mean, se ratings ~ genre * explicit information level * working memory * question
ggplot(genre_summary) + 
  geom_point(aes(question_n, mean_rating, color = condition)) + 
  geom_errorbar(aes(x = question_n, y = mean_rating, 
                   ymin = mean_rating-se_rating, ymax = mean_rating+se_rating, color = condition),
                width = 0.2) + 
  geom_line(aes(question_n, mean_rating, group = condition, color = condition)) + 
  facet_wrap(genre ~ wm, labeller = 
               as_labeller(c(`0` = "no load", `1` = "wm load", 'rock' = "rock", 'classical' = "classical"))) +
  scale_color_brewer(palette = "Dark2", name = "explicit information") +
  theme_classic() +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(y = "liking ratings", x = NULL)


# Mean liking rating ~ working memory, explicit information, genre 
# Reported as a table
genre_mu_summary <- rating_data %>% group_by(wm, condition, genre) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd_rating/sqrt(nrow(rating_data)),
                                                                median = median(rating),
                                                                IQR = IQR(rating))

sd(rating_data$rating)
mean(rating_data$rating)
```


I also want to take a look at descriptive statistics for these three IVs
```{r}
rating_wm <- rating_data %>% group_by(wm) %>% summarise(mean_rating = mean(rating),
                                                                sd_rating = sd(rating),
                                                                se_rating = sd(rating) /sqrt(nrow(rating_data)))

rating_expinf <- rating_data %>% group_by(condition) %>% summarise(mean_rating = mean(rating),
                                                                   sd_rating = sd(rating),
                                                                   se_rating = sd(rating) / sqrt(nrow(rating_data)))

rating_genre <- rating_data %>% group_by(genre) %>% summarise(mean_rating = mean(rating),
                                                                   sd_rating = sd(rating),
                                                                   se_rating = sd(rating) / sqrt(nrow(rating_data)))
```

## I.5 Familiarity ratings
```{r}
familiarity_summary <- rating_data %>% group_by(genre) %>% summarise(mean_familiarity = mean(knew_piece),
                                                                     sd_fam = sd(knew_piece))
```

## I.6 Transform Data for Ordinal Regression
Ideally, we now just have to quantify what we already saw above, using statistical modelling. We have to first make sure our data is in the right format for modelling. 

Since we are using Likert scale data, and we intend to aggregate it by taking a mean, it would not be wholly appropriate to do a linear regression.

Instead, we can use a class of logistic regression called ordinal regression, which specifically handles 
ordinal data (where there is a bounded scale with ordered levels).

We'll use a function from the package `ordinal` to model our data with both fixed and random effects.

First, our ordinal variables should be coded as ordered variables (i.e. as ordered factors), and continuous variables coded as numeric.

```{r}
data_prep <- data_or_rm %>% 
  mutate(ID = as.factor(ID),
         wm = factor(wm),
         # condition should already be a factor, ordered by "baseline", "low", "high"
         # for consistency with the paper, 
         # let's use a variable in place of `conditions` called `explicit information`:
         exp_inf = factor(case_when(condition == "baseline" ~ 2, #case_when is like an if
                             condition == "low" ~ 0,
                             condition == "high" ~ 1)),
         genre = factor(genre, levels = c("classical","rock")),
         # let's add a variable for `repeated exposure`:
         rep_exp = factor(case_when(condition == "baseline" ~ 0,
                             prime_order == 1 & condition == "high" ~ 1,
                             prime_order == 1 & condition == "low" ~ 2,
                             prime_order == 2 & condition == "high" ~ 2,
                             prime_order == 2 & condition == "low" ~ 1)))

```

Now, get the (rounded) mean ratings. (Although there's many more sophisticated and elegant ways of combining
Likert scale data, the important thing for us is to have a single value that captures a participant's 
overall liking in a way that makes sense on the original scale, i.e. is also an integer.)

```{r}
data_olm <- data_prep[!is.na(data_or_rm$rating),] %>% 
  group_by(ID, condition, genre) %>% 
  mutate(mean_rating = round(mean(rating)), .after = rating) %>% 
  ungroup() %>%
  mutate(mean_rating = factor(mean_rating, ordered = TRUE),
         rating = factor(rating, ordered = TRUE))

str(data_olm) # view data structure
# genre, wm, rating, mean_rating

```

To get a sense of the breakdown of mean ratings for each group, and to give us an intuition about
what the ordinal regression will tell us, let's visualize our data as counts:
```{r}
# Proportion of liking ratings for classical music
p_classical_olm <- data_olm %>% filter(genre == "classical") %>%
  ggplot(aes(x = factor(exp_inf), fill = factor(mean_rating))) +
  geom_bar(position = "fill") +
  facet_grid(. ~ wm, 
             labeller = as_labeller(c(`0` = "no cognitive load", `1` = "cognitive load"))) +
  scale_fill_brewer(palette = "PuBuGn") + 
  scale_x_discrete(name = "Explicit information condition", labels = c("low","high","none")) + 
  scale_y_continuous(name = "Proportion of liking ratings", breaks = seq(0.00, 1.00, 0.2)) + 
  labs(fill = "Liking rating", title = "Proportion of liking ratings for classsical music") + 
  theme_classic() +
  theme(text = element_text(size = 14),
        axis.text = element_text(size = 12),
        title = element_text(face = "bold")) 
p_classical_olm
ggsave("classical_proportion_liking.png", plot = p_classical_olm, width = 10, height = 7, units = "in")
```


```{r}
# Proportion of liking ratings for rock music
p_rock_olm <- data_olm %>% filter(genre == "rock") %>%
  ggplot(aes(x = factor(exp_inf), fill = factor(mean_rating))) +
  geom_bar(position = "fill") +
  facet_grid(. ~ wm, 
             labeller = as_labeller(c(`0` = "no cognitive load", `1` = "cognitive load"                                                   ))) +
  scale_fill_brewer(palette = "YlOrRd") + 
  scale_x_discrete(name = "Explicit information condition", labels = c("low","high","none")) + 
  scale_y_continuous(name = "Proportion of liking ratings", breaks = seq(0, 1, 0.20)) + 
  labs(fill = "Liking rating", title = "Proportion of liking ratings for rock music") + 
  theme_classic() +
  theme(text = element_text(size = 14),
        axis.text = element_text(size = 12),
        title = element_text(face = "bold")) 
p_rock_olm
ggsave("rock_proportion_liking.png", plot = p_rock_olm, width = 10, height = 7, units = "in")
```

## I.7 Ordinal Logistic Regression
Our outcome variable is the `mean_ratings` (a combination of four Likert scale liking ratings).
Our predictors are `wm` (the effect of working memory load) and `exp_inf` (the effect of levels of prestige suggestion).

Fill in the model with these terms.

We want to include the interaction of `wm` and `exp_inf`

Recall that we want to analyze the classical and rock music conditions separately. How would we do this?
```{r}
data_class <- subset(data_olm, data_olm$genre == "classical")
class_olmm <- clmm(mean_rating ~ wm + exp_inf +  wm:exp_inf + (1|ID),
                   data = data_class) 

# get the summary of the model
summary(class_olmm) 
sjPlot::tab_model(class_olmm)
Anova.clmm(class_olmm)
# odds ratio & ci
exp(cbind(OR = coef(class_olmm), confint(class_olmm)))
# The first level of each coefficient goes into the intercept

# coefficient exp_inf1 tells us change from explicit information low to high has log odds 1.145 
# This is the information we need for answering our hypothesis

# Interaction is not significant

# Second model which only includes explicit information as predictor
class_olmm2 <- clmm(mean_rating ~ exp_inf +  (1|ID), data = data_class)
summary(class_olmm2)
sjPlot::tab_model(class_olmm2)
Anova.clmm(class_olmm2)


# odds ratio & ci
exp(cbind(OR = coef(class_olmm2), confint(class_olmm2)))
```


Are the assumptions for the model fulfilled?
```{r}
cor.test(data_class$wm, data_class$exp_inf)
# No multicollinearity; predictors are not correlated because we assigned participants randomly
```

Now, what about these **threshold coefficients**? Each term is giving a sense of the likelihood of an 
observed rating at or below that threshold, 
e.g. the term `1|2` describes the likelihood of observing a 1 as opposed to a 2, 3, 4... 7; 
the term `2|3` gives the likelihood of observing a 1 or 2 as opposed to a 3, 4, 5... 7... 

Again, notice the spin that the ordinal aspect of the model brings into play: the odds are cumulative. 
We can calculate the odds of observing a rating at a certain level (and below it) for each value of each predictor variable.

Now, using `ggpredict()`, we can calculate the probabilities of getting each rating as follows. We can plot the outcome as well. (I will not report this)
```{r}
# generate predictions from the model
ggpredictions_class <- data.frame(ggpredict(class_olmm, terms = c("wm", "exp_inf"), type = "fe")) 
# fe means predictions for fixed effects

# clean up
ggpredictions_class$x = factor(ggpredictions_class$x)
colnames(ggpredictions_class)[c(1, 6,7)] <- c("wm", "mean_rating", "exp_inf")

ggpredictions_class

ggplot(ggpredictions_class, aes(x = mean_rating, y = predicted)) + 
  geom_point(aes(color = wm), position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = wm), 
                position = position_dodge(width = 0.5), width = 0.3) + 
  facet_wrap(~ exp_inf, 
               labeller = as_labeller(c(`0` = "baseline", `1` = "low", `2` = "high"))) +
  scale_fill_brewer(palette = "Set2") +
  theme_classic() 

ggplot(ggpredictions_class, aes(x = exp_inf, y = predicted, fill = mean_rating)) + 
  geom_bar(position = "fill", stat = "identity") + 
  facet_grid(. ~ wm, 
             labeller = as_labeller(c(`0` = "no load", `1` = "load"))) +
  scale_fill_brewer(palette = "Set2") + 
  scale_x_discrete(labels = c("low","high","baseline")) + 
  theme_classic() +
  ggtitle("Probabilities of responses, full model")
```

Now, repeat the above analysis with the rock music condition:
```{r}
data_rock <- subset(data_olm, data_olm$genre == "rock")
rock_olmm <- clmm(mean_rating ~ wm + exp_inf +  wm:exp_inf + (1|ID), 
                   data = data_rock) 
summary(rock_olmm)

# odds ratio & ci
exp(cbind(OR = coef(rock_olmm), confint(rock_olmm)))
sjPlot::tab_model(rock_olmm)
Anova.clmm(rock_olmm)

# Second model which only includes explicit information as predictor
rock_olmm2 <- clmm(mean_rating ~ exp_inf +   (1|ID), data = data_rock) 
summary(rock_olmm2)
exp(cbind(OR = coef(rock_olmm2), confint(rock_olmm2)))
sjPlot::tab_model(rock_olmm2)
anova(rock_olmm, rock_olmm2)
RVAideMemoire::Anova.clmm(rock_olmm2)

```


```{r}
# generate predictions from the model
ggpredictions_rock <- data.frame(ggpredict(rock_olmm, terms = c("wm", "exp_inf"), type = "fe"))

# clean up
ggpredictions_rock$x = factor(ggpredictions_rock$x)
colnames(ggpredictions_rock)[c(1, 6,7)] <- c("wm", "mean_rating", "exp_inf")

ggpredictions_rock

ggplot(ggpredictions_rock, aes(x = mean_rating, y = predicted)) + 
  geom_point(aes(color = wm), position = position_dodge(width = 0.5)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high, color = wm), 
                position = position_dodge(width = 0.5), width = 0.3) + 
  facet_wrap(~ exp_inf, 
               labeller = as_labeller(c(`0` = "baseline", `1` = "low", `2` = "high"))) +
  #scale_fill_brewer(palette = "Set2") +
  theme_classic() 

ggplot(ggpredictions_rock, aes(x = exp_inf, y = predicted, fill = mean_rating)) + 
  geom_bar(position = "fill", stat = "identity") + 
  facet_grid(. ~ wm, 
             labeller = as_labeller(c(`0` = "no load", `1` = "load"))) +
  scale_fill_brewer(palette = "Set2") + 
  scale_x_discrete(labels = c("low","high","baseline")) + 
  theme_classic() +
  ggtitle("Probabilities of responses, full model")
```


# Analysis II
Did cognitive load, genre or musical abilities affect the rate at which participants fell for the repeated recording illusion?

Aim
- Generate a contingency table of participants who fell/did not fall for the illusion, broken down by condition (`genre`,`working memory load`,`explicit information`)
- Perform a Chi-square to determine if more people fell for the illusion under certain treatments
- (Optionally) Perform a logistic regression to determine the likelihood of falling for the illusion, as a function of different variables

### II.1 Contigency Table
At first, I want to organize the data in a way so that I can report it in a contigency table. This is also necessary for later inferential tests using the Chisquared test.

We will use a new data set [chisq_data] which includes only 84 participants because we had to exclude the whole musical condition if a cognitive load trial was failed (we can not compare the ratings between explicit information conditions if we have previously deleated one or more expinf conditions). Additionally, we had to exclude 3 further participants who gave nonsensible answers in the open text boxes.
```{r}
chisq_data <- read.csv(file.path(data_path, "classified_data_3b.csv"), 
                       header = T, sep = ";")
print(paste("N =", length(unique(chisq_data$ID))))

chisq_data$Fell <- factor(chisq_data$Fell, labels = c("no", "yes"))
chisq_data$wm <- factor(chisq_data$wm, labels = c("no load", "load"))

sum(chisq_data$Fell == "yes") / length(chisq_data$Fell)
# Participants fell for the illusion in 53.6% of trials


#How many participants fell at least once for the illusion? (Table row 1)
length(unique(chisq_data$ID[chisq_data$Fell == "yes"]))
# 55 participants fell for the illusion

#Falling for the illusion per genre (Row 4 and 5)
illusion_genre <- table(chisq_data$genre, chisq_data$Fell)
illusion_genre
chisq.test(illusion_genre, correct = F)

#How many participants fell in the cognitive load group? (Table row 2)
chisq_data_wm <- chisq_data %>% filter(wm == "load")
length(unique(chisq_data_wm$ID[chisq_data_wm$Fell == "yes"]))
length(unique(chisq_data_wm$ID))
#23 out of 40 participants in the cognitive load group fell for the illusion

# How many participants fell for the illusion in the no load group? (Table row 3)
chisq_data_nowm <- chisq_data %>% filter(wm == "no load")
length(unique(chisq_data_nowm$ID[chisq_data_nowm$Fell == "yes"]))
length(unique(chisq_data_nowm$ID))
#32 out of 44 participants in the cognitive load group fell for the illusion

# Is there an association between cognitive load group and occurrence of the illusion?
mat <- matrix(nrow = 2, ncol = 2, c(23, 32, 17, 12), 
              dimnames = list(c("load", "no load"), c("yes", "no")))
chisq.test(mat, correct = F)
```


Aim
- Examine influence of musical sophistication on the occurrence of the RRI

### II.2 RRI & Musical abilities
1. Load data and merge data sets

We used two Scales from the GMSI: Perceptual Abilities (PA), Musical Training (MT). Based on the average of 7 items from these two scales a  score of General Musical Abilities (GM) was calculated. 
```{r}
data2 <- read.csv(file.path(data_path,'scored_data.csv'))
GSI_data <- subset(data2, select = c(uid, sex, age, PA_mean, MT_mean, GM_mean)) 
            %>% distinct()
chisq_data

m1 <- merge(chisq_data, GSI_data, by.x = "ID", by.y = "uid") %>% distinct()

m1_genre_bin <- m1 %>% group_by(genre, wm, GM_mean) %>%
                       mutate(GM_bin = case_when(GM_mean < 3.5 ~ "low", 
                                                 # low general music ability score
                                                 GM_mean > 3.5 ~ "high"))
# high score
  
hist(m1_genre_bin$GM_mean)

summary(glm(Fell ~ genre + wm +  GM_mean, family = binomial(link='logit'), data = m1))


#Row 6: How many participants fell in the high musical abilities group?
chisq_data_high <- m1_genre_bin %>% filter(GM_bin == "high")
length(unique(chisq_data_high$ID[chisq_data_high$Fell == "yes"]))
length(unique(chisq_data_high$ID))

#Row 7: How many participants fell in the low musical abilities group?
chisq_data_low <- m1_genre_bin %>% filter(GM_bin == "low")
length(unique(chisq_data_low$ID[chisq_data_low$Fell == "yes"]))
length(unique(chisq_data_low$ID))

# Is there an association between musical abilities and occurrence of the illusion?
mat2 <- matrix(nrow = 2, ncol = 2, c(36, 19, 19, 10), 
               dimnames = list(c("high", "low"), c("yes", "no")))
chisq.test(mat2, correct = F)
# No!



model1 <- glm(Fell ~ genre + wm +  GM_mean, family = binomial(link='logit'), data = m1)
summary(model1)
car::Anova(model1, type = 2)

# Deviance analysis gives us each factor independently does it tell us about the variance in the data?
# The original paper found also no effect of genre on yes/no illusion and also found no effect of prestige
# This is coherent with the findings 
```

We can also try and model the likelihood of falling for the RRI as a binomial logistic regression. Let's do so with `genre` , `wm` and `GM_mean` as predictors.
```{r}
bin_model <- glm(Fell ~ genre + wm +  GM_mean, family = binomial(link='logit'), data = m1)
summary(bin_model)
car::Anova(bin_model, type = 2)
exp( cbind(odd_ratio = coef(bin_model), confint(bin_model,level = .95)) )
```
